{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMqAKJOQziBZfYpRysi3Phd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jundula/Neural-networks/blob/main/ARISOY_ABD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Group Project**"
      ],
      "metadata": {
        "id": "dkVGPn_o8cGj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Step 1: Load Stock Data**"
      ],
      "metadata": {
        "id": "M3H_rGAH8g8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "stock_data_path = \"data.csv\"  # File uploaded to Colab\n",
        "stock_data = pd.read_csv(stock_data_path)\n",
        "\n",
        "# Convert date column to datetime format\n",
        "stock_data['date'] = pd.to_datetime(stock_data['date'])\n",
        "\n",
        "# Filter data: Prices > $1, Exchange codes 1, 2, or 3, Share codes 10 or 11\n",
        "filtered_data = stock_data[\n",
        "    (stock_data['PRC'].abs() > 1) &\n",
        "    (stock_data['EXCHCD'].isin([1, 2, 3])) &\n",
        "    (stock_data['SHRCD'].isin([10, 11]))\n",
        "]\n",
        "\n",
        "# Calculate market equity (ME)\n",
        "filtered_data['ME'] = filtered_data['PRC'].abs() * filtered_data['SHROUT']\n",
        "\n",
        "# Sort by PERMNO and date\n",
        "filtered_data = filtered_data.sort_values(by=['PERMNO', 'date'])"
      ],
      "metadata": {
        "id": "cwhHvR6G82PC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 2: Load Fama-French Factors**"
      ],
      "metadata": {
        "id": "8Y6lPPhX83Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ff_factors_path = \"F-F_Research_Data_Factors.csv\"  # File uploaded to Colab\n",
        "\n",
        "# Skip the first 3 rows (descriptive text) and use row 4 as the header\n",
        "ff_factors = pd.read_csv(ff_factors_path, skiprows=3)\n",
        "\n",
        "# Assign a label to the first column (date column)\n",
        "ff_factors.columns = ['date'] + list(ff_factors.columns[1:])\n",
        "\n",
        "# Drop rows where the 'date' column is NaN\n",
        "ff_factors = ff_factors.dropna(subset=['date'])\n",
        "\n",
        "# Filter out rows where the 'date' column does not match the YYYYMM format\n",
        "ff_factors = ff_factors[ff_factors['date'].str.isdigit() & (ff_factors['date'].str.len() == 6)]\n",
        "\n",
        "# Convert date column to datetime format\n",
        "ff_factors['date'] = pd.to_datetime(ff_factors['date'], format='%Y%m')\n",
        "\n",
        "# Rename columns for consistency\n",
        "ff_factors.rename(columns={'Mkt-RF': 'mkt_excess_return'}, inplace=True)\n",
        "\n",
        "# Ensure the dates align with the stock data\n",
        "ff_factors = ff_factors.set_index('date').resample('ME').last().reset_index()"
      ],
      "metadata": {
        "id": "6bwrfQ0C9JjC"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 3: Load q-Factor Data**"
      ],
      "metadata": {
        "id": "3QQTbRJO9L8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_factors_path = \"q5_factors_monthly_2023.csv\"  # File uploaded to Colab\n",
        "q_factors = pd.read_csv(q_factors_path)\n",
        "\n",
        "# Combine year and month into a single date column\n",
        "q_factors['date'] = pd.to_datetime(q_factors['year'].astype(str) + '-' + q_factors['month'].astype(str) + '-01')\n",
        "\n",
        "# Rename columns for consistency (Replace 'R_ME' with your assigned factor)\n",
        "q_factors.rename(columns={'R_MKT': 'mkt_excess_return', 'R_ME': 'allocated_factor'}, inplace=True)\n",
        "\n",
        "# Drop unnecessary columns (year and month), but keep 'date', 'mkt_excess_return', and 'allocated_factor'\n",
        "q_factors = q_factors[['date', 'mkt_excess_return', 'allocated_factor']]\n",
        "\n",
        "# Ensure the dates align with the stock data\n",
        "q_factors = q_factors.set_index('date').resample('ME').last().reset_index()"
      ],
      "metadata": {
        "id": "ogjg_8HX9Xgk"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 4: Merge Stock Data with Factors**"
      ],
      "metadata": {
        "id": "fWmU8PRO9e61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_data = pd.merge(filtered_data, ff_factors, on='date', how='inner')\n",
        "merged_data = pd.merge(merged_data, q_factors, on='date', how='inner')\n",
        "\n",
        "# Resolve duplicate columns if necessary\n",
        "if 'mkt_excess_return_x' in merged_data.columns and 'mkt_excess_return_y' in merged_data.columns:\n",
        "    merged_data['mkt_excess_return'] = merged_data['mkt_excess_return_x']\n",
        "    merged_data.drop(columns=['mkt_excess_return_x', 'mkt_excess_return_y'], inplace=True)\n",
        "\n",
        "# Debug: Print columns after merging\n",
        "print(\"Merged Data Columns:\", merged_data.columns)\n",
        "\n",
        "# Ensure RET and RF columns are numeric\n",
        "merged_data['RET'] = pd.to_numeric(merged_data['RET'], errors='coerce')\n",
        "merged_data['RF'] = pd.to_numeric(merged_data['RF'], errors='coerce')\n",
        "\n",
        "# Calculate excess returns for each stock\n",
        "merged_data['excess_return'] = merged_data['RET'] - merged_data['RF']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGHQf8U59Zxk",
        "outputId": "a36829b9-defb-4118-9ee2-28e2df77e6c6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged Data Columns: Index(['PERMNO', 'date', 'SHRCD', 'EXCHCD', 'PRC', 'RET', 'SHROUT', 'ME',\n",
            "       'SMB', 'HML', 'RF', 'allocated_factor', 'mkt_excess_return'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 5: Create a Smaller Subset for Testing**"
      ],
      "metadata": {
        "id": "Nd4hZrz0_IJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit the date range to a shorter period (e.g., 1973–1983)\n",
        "subset_data = merged_data\n",
        "\n",
        "# Randomly select a subset of PERMNOs (e.g., 100 stocks)\n",
        "unique_permnos = subset_data['PERMNO'].unique()\n",
        "sample_permnos = np.random.choice(unique_permnos, size=100, replace=False)  # Randomly select 100 PERMNOs\n",
        "subset_data = subset_data[subset_data['PERMNO'].isin(sample_permnos)]\n",
        "\n",
        "# Verify the subset\n",
        "print(f\"Subset Data Shape: {subset_data.shape}\")\n",
        "print(f\"Unique PERMNOs in Subset: {len(subset_data['PERMNO'].unique())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJVZExPM9lyr",
        "outputId": "10631332-a2a1-4a5e-bc68-bf67057443f0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset Data Shape: (9209, 14)\n",
            "Unique PERMNOs in Subset: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 6: Rolling Regression Function (Using TensorFlow for GPU Acceleration)**"
      ],
      "metadata": {
        "id": "YLQnzkVx98gN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_regression_tensorflow(data, window=60):  # Use full window size for testing\n",
        "    betas_mkt = []\n",
        "    betas_allocated = []\n",
        "\n",
        "    for i in range(window, len(data)):\n",
        "        # Extract the rolling window\n",
        "        window_data = data.iloc[i-window:i]\n",
        "\n",
        "        # Drop rows with NaN values in any of the required columns\n",
        "        window_data = window_data.dropna(subset=['mkt_excess_return', 'allocated_factor', 'excess_return'])\n",
        "\n",
        "        # Skip if there are fewer than 2 observations (minimum required for regression)\n",
        "        if len(window_data) < 2:\n",
        "            betas_mkt.append(np.nan)\n",
        "            betas_allocated.append(np.nan)\n",
        "            continue\n",
        "\n",
        "        # Define independent variables (MKT and Allocated Factor) and dependent variable (excess return)\n",
        "        X = window_data[['mkt_excess_return', 'allocated_factor']].values\n",
        "        y = window_data['excess_return'].values\n",
        "\n",
        "        try:\n",
        "            # Convert to TensorFlow tensors and move to GPU\n",
        "            X_tf = tf.convert_to_tensor(X, dtype=tf.float32)\n",
        "            y_tf = tf.convert_to_tensor(y, dtype=tf.float32)\n",
        "\n",
        "            # Reshape y_tf to be a 2D array with shape (n_samples, 1)\n",
        "            y_tf = tf.reshape(y_tf, (-1, 1))\n",
        "\n",
        "            # Add a column of ones for the intercept term\n",
        "            X_tf = tf.concat([tf.ones((X_tf.shape[0], 1), dtype=tf.float32), X_tf], axis=1)\n",
        "\n",
        "            # Compute coefficients using the normal equation: β = (X^T X)^(-1) X^T y\n",
        "            beta = tf.linalg.inv(tf.transpose(X_tf) @ X_tf) @ tf.transpose(X_tf) @ y_tf\n",
        "\n",
        "            # Save the betas (skip the intercept term)\n",
        "            betas_mkt.append(float(beta[1].numpy().item()))  # Ensure scalar value for MKT\n",
        "            betas_allocated.append(float(beta[2].numpy().item()))  # Ensure scalar value for Allocated Factor\n",
        "        except Exception as e:\n",
        "            print(f\"Error in regression at index {i}: {e}\")\n",
        "            betas_mkt.append(np.nan)\n",
        "            betas_allocated.append(np.nan)\n",
        "\n",
        "    # Return betas starting from the 61st month (window + 1)\n",
        "    return pd.DataFrame({\n",
        "        'date': data['date'][window:],\n",
        "        'beta_mkt': betas_mkt,\n",
        "        'beta_allocated': betas_allocated\n",
        "    })\n"
      ],
      "metadata": {
        "id": "kMi4co9Y94rb"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 7: Apply Rolling Regression to Each Stock in the Subset**"
      ],
      "metadata": {
        "id": "ts-YA0kg_H3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = subset_data.groupby('PERMNO')\n",
        "results_subset = grouped.apply(lambda x: rolling_regression_tensorflow(x, window=60))  # Use full window size\n",
        "\n",
        "# Reset index to flatten the results\n",
        "results_subset = results_subset.reset_index(level=0).rename(columns={'level_0': 'PERMNO'})\n",
        "\n",
        "# Inspect the results\n",
        "print(\"Subset Results Shape:\", results_subset.shape)\n",
        "print(results_subset.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "xG7zSf9Y-JfT",
        "outputId": "c9b4d057-47d0-477c-f138-e604c3491078"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-bbb6fddb9e85>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubset_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PERMNO'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrolling_regression_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use full window size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Reset index to flatten the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'level_0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'PERMNO'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, include_groups, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0moption_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode.chained_assignment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1824\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1825\u001b[0m                 if (\n\u001b[1;32m   1826\u001b[0m                     \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_python_apply_general\u001b[0;34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[0m\n\u001b[1;32m   1883\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \"\"\"\n\u001b[0;32m-> 1885\u001b[0;31m         \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_groupwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1886\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnot_indexed_same\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m             \u001b[0mnot_indexed_same\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36mapply_groupwise\u001b[0;34m(self, f, data, axis)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# group might be modified\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmutated\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-bbb6fddb9e85>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgrouped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubset_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PERMNO'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrouped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrolling_regression_tensorflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use full window size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Reset index to flatten the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults_subset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_subset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'level_0'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'PERMNO'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-da4db351adc1>\u001b[0m in \u001b[0;36mrolling_regression_tensorflow\u001b[0;34m(data, window)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Define independent variables (MKT and Allocated Factor) and dependent variable (excess return)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mkt_excess_return'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'allocated_factor'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwindow_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'excess_return'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4107\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4108\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4110\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6195\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6196\u001b[0;31m             \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6197\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, target, method, level, limit, tolerance)\u001b[0m\n\u001b[1;32m   4420\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4422\u001b[0;31m                     indexer = self.get_indexer(\n\u001b[0m\u001b[1;32m   4423\u001b[0m                         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4424\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3888\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3890\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_partial_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3891\u001b[0m             \u001b[0;31m# IntervalIndex get special treatment bc numeric scalars can be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3892\u001b[0m             \u001b[0;31m#  matched to Interval scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_should_compare\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   6410\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6412\u001b[0;31m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpack_nested_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6413\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_comparable_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_unpack_nested_dtype\u001b[0;34m(other)\u001b[0m\n\u001b[1;32m   7721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7723\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_unpack_nested_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDtypeObj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7724\u001b[0m     \"\"\"\n\u001b[1;32m   7725\u001b[0m     \u001b[0mWhen\u001b[0m \u001b[0mchecking\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mour\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mcomparable\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mneed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 8: Portfolio Formation (Monthly Deciles)**"
      ],
      "metadata": {
        "id": "bErsEhER_H9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Portfolio Formation (Monthly Deciles)\n",
        "def form_portfolios(data, beta_column, weight_column=None):\n",
        "    \"\"\"\n",
        "    Forms 10 decile portfolios based on beta values.\n",
        "    If weight_column is None, portfolios are equally-weighted.\n",
        "    Otherwise, portfolios are value-weighted using the weight_column.\n",
        "    \"\"\"\n",
        "    # Ensure beta_column contains valid scalar values\n",
        "    data[beta_column] = pd.to_numeric(data[beta_column], errors='coerce')\n",
        "\n",
        "    # Check if there are enough unique beta values for decile ranking\n",
        "    unique_betas = data[beta_column].dropna().unique()\n",
        "    if len(unique_betas) < 10:\n",
        "        print(f\"Not enough unique beta values ({len(unique_betas)}) to form 10 deciles.\")\n",
        "        return pd.Series({i: np.nan for i in range(10)})\n",
        "\n",
        "    # Rank stocks into deciles based on beta values\n",
        "    try:\n",
        "        data['decile'] = pd.qcut(data[beta_column], q=10, labels=False)\n",
        "    except ValueError as e:\n",
        "        print(f\"Error in pd.qcut: {e}\")\n",
        "        return pd.Series({i: np.nan for i in range(10)})\n",
        "\n",
        "    # Group by decile and calculate portfolio returns\n",
        "    if weight_column:\n",
        "        # Value-weighted returns\n",
        "        portfolio_returns = data.groupby('decile', group_keys=False)[['RET', weight_column]].apply(\n",
        "            lambda x: np.average(x['RET'], weights=x[weight_column])\n",
        "        )\n",
        "    else:\n",
        "        # Equally-weighted returns\n",
        "        portfolio_returns = data.groupby('decile', group_keys=False)['RET'].mean()\n",
        "\n",
        "    return portfolio_returns"
      ],
      "metadata": {
        "id": "HMeGOLeU-NDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 9: Monthly Portfolio Returns**"
      ],
      "metadata": {
        "id": "xcf0gFW49t3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge betas with stock data\n",
        "portfolio_data_subset = pd.merge(results_subset, subset_data, on=['PERMNO', 'date'], how='inner')\n",
        "\n",
        "# Form portfolios monthly\n",
        "monthly_portfolios_equal_weight = portfolio_data_subset.groupby('date').apply(\n",
        "    lambda x: form_portfolios(x, beta_column='beta_allocated', weight_column=None)\n",
        ")\n",
        "\n",
        "monthly_portfolios_value_weight = portfolio_data_subset.groupby('date').apply(\n",
        "    lambda x: form_portfolios(x, beta_column='beta_allocated', weight_column='ME')\n",
        ")\n",
        "\n",
        "# Inspect the portfolio returns\n",
        "print(\"Monthly Portfolios (Equally-Weighted):\")\n",
        "print(monthly_portfolios_equal_weight.head())\n",
        "\n",
        "print(\"Monthly Portfolios (Value-Weighted):\")\n",
        "print(monthly_portfolios_value_weight.head())"
      ],
      "metadata": {
        "id": "jVo5APke-M3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 10: Arbitrage Portfolio**"
      ],
      "metadata": {
        "id": "aJ4H0Uxt_IVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Arbitrage Portfolio (Long Portfolio 9, Short Portfolio 0)\n",
        "arbitrage_portfolio_equal_weight = monthly_portfolios_equal_weight[9] - monthly_portfolios_equal_weight[0]\n",
        "arbitrage_portfolio_value_weight = monthly_portfolios_value_weight[9] - monthly_portfolios_value_weight[0]\n",
        "\n",
        "# Save arbitrage portfolio returns to CSV files\n",
        "arbitrage_portfolio_equal_weight.to_csv(\"arbitrage_portfolio_equal_weight.csv\")\n",
        "arbitrage_portfolio_value_weight.to_csv(\"arbitrage_portfolio_value_weight.csv\")"
      ],
      "metadata": {
        "id": "iq8lSfsU-SFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 11: Performance Metrics**"
      ],
      "metadata": {
        "id": "o_Y8uBcc_qHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 11: Performance Metrics\n",
        "def calculate_metrics(returns):\n",
        "    avg_return = returns.mean()\n",
        "    std_return = returns.std()\n",
        "    sharpe_ratio = avg_return / std_return\n",
        "    t_stat = avg_return / (std_return / np.sqrt(len(returns)))\n",
        "    return pd.Series({\n",
        "        'Average Return': avg_return,\n",
        "        'Standard Deviation': std_return,\n",
        "        'Sharpe Ratio': sharpe_ratio,\n",
        "        't-statistic': t_stat\n",
        "    })\n",
        "\n",
        "# Calculate metrics for equally-weighted arbitrage portfolio\n",
        "if arbitrage_portfolio_equal_weight.notna().sum() > 0:\n",
        "    metrics_equal_weight = calculate_metrics(arbitrage_portfolio_equal_weight)\n",
        "    print(\"Metrics for Equally-Weighted Arbitrage Portfolio:\")\n",
        "    print(metrics_equal_weight)\n",
        "else:\n",
        "    print(\"Equally-weighted arbitrage portfolio contains no valid returns.\")\n",
        "    metrics_equal_weight = pd.Series({\n",
        "        'Average Return': np.nan,\n",
        "        'Standard Deviation': np.nan,\n",
        "        'Sharpe Ratio': np.nan,\n",
        "        't-statistic': np.nan\n",
        "    })\n",
        "\n",
        "metrics_equal_weight.to_csv(\"arbitrage_portfolio_metrics_equal_weight.csv\")\n",
        "\n",
        "# Calculate metrics for value-weighted arbitrage portfolio\n",
        "if arbitrage_portfolio_value_weight.notna().sum() > 0:\n",
        "    metrics_value_weight = calculate_metrics(arbitrage_portfolio_value_weight)\n",
        "    print(\"Metrics for Value-Weighted Arbitrage Portfolio:\")\n",
        "    print(metrics_value_weight)\n",
        "else:\n",
        "    print(\"Value-weighted arbitrage portfolio contains no valid returns.\")\n",
        "    metrics_value_weight = pd.Series({\n",
        "        'Average Return': np.nan,\n",
        "        'Standard Deviation': np.nan,\n",
        "        'Sharpe Ratio': np.nan,\n",
        "        't-statistic': np.nan\n",
        "    })\n",
        "\n",
        "metrics_value_weight.to_csv(\"arbitrage_portfolio_metrics_value_weight.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "2cBF6YZ72lQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 12: CAPM Alpha Calculation**"
      ],
      "metadata": {
        "id": "tkdQNd5s_HyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_capm_alpha(portfolio_returns, market_returns):\n",
        "    \"\"\"\n",
        "    Calculate CAPM alpha and beta using linear regression.\n",
        "    \"\"\"\n",
        "    X = market_returns.values.reshape(-1, 1)  # Independent variable (MKT)\n",
        "    y = portfolio_returns.values               # Dependent variable (portfolio returns)\n",
        "\n",
        "    model = LinearRegression().fit(X, y)\n",
        "    alpha = model.intercept_                   # CAPM alpha\n",
        "    beta = model.coef_[0]                      # CAPM beta\n",
        "    residuals = y - model.predict(X)           # Residuals\n",
        "    alpha_std_error = np.std(residuals) / np.sqrt(len(residuals))  # Standard error of alpha\n",
        "    t_statistic = alpha / alpha_std_error      # t-statistic for alpha\n",
        "\n",
        "    return pd.Series({\n",
        "        'CAPM Alpha': alpha,\n",
        "        'CAPM Beta': beta,\n",
        "        't-statistic': t_statistic\n",
        "    })\n",
        "\n",
        "# Merge portfolio returns with Fama-French factors\n",
        "monthly_portfolios_equal_weight = monthly_portfolios_equal_weight.reset_index()\n",
        "monthly_portfolios_value_weight = monthly_portfolios_value_weight.reset_index()\n",
        "\n",
        "portfolio_returns_with_factors = pd.merge(\n",
        "    monthly_portfolios_equal_weight.melt(id_vars='date', var_name='decile', value_name='return'),\n",
        "    ff_factors[['date', 'mkt_excess_return']],\n",
        "    on='date',\n",
        "    how='inner'\n",
        ")\n"
      ],
      "metadata": {
        "id": "JY42ALhRB5bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate CAPM alpha for all 10 portfolios (equally-weighted)\n",
        "capm_results_equal_weight = portfolio_returns_with_factors.groupby('decile').apply(\n",
        "    lambda x: calculate_capm_alpha(x['return'], x['mkt_excess_return'])\n",
        ")\n",
        "\n",
        "print(\"CAPM Results for Equally-Weighted Portfolios:\")\n",
        "print(capm_results_equal_weight)"
      ],
      "metadata": {
        "id": "IJWPQtsCB9WU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate CAPM alpha for all 10 portfolios (value-weighted)\n",
        "portfolio_returns_with_factors_value_weight = pd.merge(\n",
        "    monthly_portfolios_value_weight.melt(id_vars='date', var_name='decile', value_name='return'),\n",
        "    ff_factors[['date', 'mkt_excess_return']],\n",
        "    on='date',\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "capm_results_value_weight = portfolio_returns_with_factors_value_weight.groupby('decile').apply(\n",
        "    lambda x: calculate_capm_alpha(x['return'], x['mkt_excess_return'])\n",
        ")\n",
        "\n",
        "print(\"CAPM Results for Value-Weighted Portfolios:\")\n",
        "print(capm_results_value_weight)"
      ],
      "metadata": {
        "id": "6pEAQ3drCBUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 13: CAPM Alpha and t-Statistics for Arbitrage**"
      ],
      "metadata": {
        "id": "MeNWCtB6CaAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge arbitrage portfolio returns with Fama-French factors\n",
        "arbitrage_portfolio_with_factors_equal_weight = pd.merge(\n",
        "    arbitrage_portfolio_equal_weight.rename('return').reset_index(),\n",
        "    ff_factors[['date', 'mkt_excess_return']],\n",
        "    on='date',\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "arbitrage_portfolio_with_factors_value_weight = pd.merge(\n",
        "    arbitrage_portfolio_value_weight.rename('return').reset_index(),\n",
        "    ff_factors[['date', 'mkt_excess_return']],\n",
        "    on='date',\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Calculate CAPM alpha for equally-weighted arbitrage portfolio\n",
        "capm_arbitrage_equal_weight = calculate_capm_alpha(\n",
        "    arbitrage_portfolio_with_factors_equal_weight['return'],\n",
        "    arbitrage_portfolio_with_factors_equal_weight['mkt_excess_return']\n",
        ")\n",
        "\n",
        "print(\"CAPM Results for Equally-Weighted Arbitrage Portfolio:\")\n",
        "print(capm_arbitrage_equal_weight)\n",
        "\n"
      ],
      "metadata": {
        "id": "_dLp0QK6COcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate CAPM alpha for value-weighted arbitrage portfolio\n",
        "capm_arbitrage_value_weight = calculate_capm_alpha(\n",
        "    arbitrage_portfolio_with_factors_value_weight['return'],\n",
        "    arbitrage_portfolio_with_factors_value_weight['mkt_excess_return']\n",
        ")\n",
        "\n",
        "print(\"CAPM Results for Value-Weighted Arbitrage Portfolio:\")\n",
        "print(capm_arbitrage_value_weight)"
      ],
      "metadata": {
        "id": "C7WycX3DChFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 14: Report Results**"
      ],
      "metadata": {
        "id": "AuMB768oCt0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average raw returns for equally-weighted portfolios\n",
        "average_raw_returns_equal_weight = monthly_portfolios_equal_weight.mean()\n",
        "\n",
        "# Average raw returns for value-weighted portfolios\n",
        "average_raw_returns_value_weight = monthly_portfolios_value_weight.mean()\n",
        "\n",
        "print(\"Average Raw Returns for Equally-Weighted Portfolios:\")\n",
        "print(average_raw_returns_equal_weight)\n",
        "\n",
        "print(\"Average Raw Returns for Value-Weighted Portfolios:\")\n",
        "print(average_raw_returns_value_weight)"
      ],
      "metadata": {
        "id": "En_17DR3CpQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine CAPM results and average raw returns for equally-weighted portfolios\n",
        "results_equal_weight = pd.concat(\n",
        "    [average_raw_returns_equal_weight.rename('Average Raw Return'), capm_results_equal_weight],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Combine CAPM results and average raw returns for value-weighted portfolios\n",
        "results_value_weight = pd.concat(\n",
        "    [average_raw_returns_value_weight.rename('Average Raw Return'), capm_results_value_weight],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"Results for Equally-Weighted Portfolios:\")\n",
        "print(results_equal_weight)\n",
        "\n",
        "print(\"Results for Value-Weighted Portfolios:\")\n",
        "print(results_value_weight)\n",
        "\n",
        "# Save results for equally-weighted portfolios\n",
        "results_equal_weight.to_csv(\"portfolio_results_equal_weight.csv\")\n",
        "\n",
        "# Save results for value-weighted portfolios\n",
        "results_value_weight.to_csv(\"portfolio_results_value_weight.csv\")\n",
        "\n",
        "# Save CAPM results for arbitrage portfolios\n",
        "capm_arbitrage_equal_weight.to_csv(\"capm_arbitrage_equal_weight.csv\")\n",
        "capm_arbitrage_value_weight.to_csv(\"capm_arbitrage_value_weight.csv\")"
      ],
      "metadata": {
        "id": "NJKqYPqECOTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Step 15: Graphs and Supplementary Tables**"
      ],
      "metadata": {
        "id": "L0zVFN74DEmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot equally-weighted portfolio returns\n",
        "plt.figure(figsize=(12, 6))\n",
        "for decile in range(10):\n",
        "    plt.plot(monthly_portfolios_equal_weight['date'], monthly_portfolios_equal_weight[decile], label=f\"Portfolio {decile}\")\n",
        "plt.title(\"Equally-Weighted Portfolio Returns\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Return\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KT956-ouDKUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot value-weighted portfolio returns\n",
        "plt.figure(figsize=(12, 6))\n",
        "for decile in range(10):\n",
        "    plt.plot(monthly_portfolios_value_weight['date'], monthly_portfolios_value_weight[decile], label=f\"Portfolio {decile}\")\n",
        "plt.title(\"Value-Weighted Portfolio Returns\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Return\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rgF9y_ObDJ_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram for equally-weighted arbitrage portfolio\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(arbitrage_portfolio_equal_weight.dropna(), bins=30, color='blue', alpha=0.7)\n",
        "plt.title(\"Histogram of Equally-Weighted Arbitrage Portfolio Returns\")\n",
        "plt.xlabel(\"Return\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nLFMbagdDNeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Histogram for value-weighted arbitrage portfolio\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(arbitrage_portfolio_value_weight.dropna(), bins=30, color='green', alpha=0.7)\n",
        "plt.title(\"Histogram of Value-Weighted Arbitrage Portfolio Returns\")\n",
        "plt.xlabel(\"Return\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L_K4bOSN4Xnv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}